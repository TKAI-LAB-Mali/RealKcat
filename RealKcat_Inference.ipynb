{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KOL28oURVuxt"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "#Input Type(s): Texts (Protein Sequences and Substrates)\n",
        "\n",
        "#Input Parameters: 1D\n",
        "\n",
        "#Other Properties Related to Input: Protein sequence represented\n",
        "#as a string of canonical amino acids, of maximum length 1022.\n",
        "#Longer sequences are automatically truncated to this length.\n",
        "\n",
        "# For substrate maximum sequence length is 512\n",
        "\n",
        "\n",
        "# Define sequences and substrates\n",
        "sequence_1 = 'MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH'  #@param {type:\"string\"}\n",
        "substrate_1 = 'C(C1C(C(C(C(O1)O)O)O)O)O'  #@param {type:\"string\"}\n",
        "\n",
        "sequence_2 = ''  #@param {type:\"string\"}\n",
        "substrate_2 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_3 = ''  #@param {type:\"string\"}\n",
        "substrate_3 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_4 = ''  #@param {type:\"string\"}\n",
        "substrate_4 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_5 = ''  #@param {type:\"string\"}\n",
        "substrate_5 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_6 = ''  #@param {type:\"string\"}\n",
        "substrate_6 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_7 = ''  #@param {type:\"string\"}\n",
        "substrate_7 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_8 = ''  #@param {type:\"string\"}\n",
        "substrate_8 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_9 = ''  #@param {type:\"string\"}\n",
        "substrate_9 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_10 = ''  #@param {type:\"string\"}\n",
        "substrate_10 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_11 = ''  #@param {type:\"string\"}\n",
        "substrate_11 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_12 = ''  #@param {type:\"string\"}\n",
        "substrate_12 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_13 = ''  #@param {type:\"string\"}\n",
        "substrate_13 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_14 = ''  #@param {type:\"string\"}\n",
        "substrate_14 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_15 = ''  #@param {type:\"string\"}\n",
        "substrate_15 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_16 = ''  #@param {type:\"string\"}\n",
        "substrate_16 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_17 = ''  #@param {type:\"string\"}\n",
        "substrate_17 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_18 = ''  #@param {type:\"string\"}\n",
        "substrate_18 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_19 = ''  #@param {type:\"string\"}\n",
        "substrate_19 = ''  #@param {type:\"string\"}\n",
        "\n",
        "sequence_20 = ''  #@param {type:\"string\"}\n",
        "substrate_20 = ''  #@param {type:\"string\"}\n",
        "# input_sequences = (\n",
        "#     sequence_1, sequence_2, sequence_3, sequence_4, sequence_5,\n",
        "#     sequence_6, sequence_7, sequence_8, sequence_9, sequence_10,\n",
        "#     sequence_11, sequence_12, sequence_13, sequence_14, sequence_15,\n",
        "#     sequence_16, sequence_17, sequence_18, sequence_19, sequence_20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fair-esm\n",
        "\n",
        "# Load ESM-2 model\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model.eval()  # Disable dropout for deterministic results\n",
        "\n",
        "# Load ChemBERT model\n",
        "chembert_model = AutoModel.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "chembert_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
        "chembert_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Prepare input data with both sequences and substrates\n",
        "input_data = [\n",
        "    (\"sequence_1\", sequence_1, substrate_1), (\"sequence_2\", sequence_2, substrate_2),\n",
        "    (\"sequence_3\", sequence_3, substrate_3), (\"sequence_4\", sequence_4, substrate_4),\n",
        "    (\"sequence_5\", sequence_5, substrate_5), (\"sequence_6\", sequence_6, substrate_6),\n",
        "    (\"sequence_7\", sequence_7, substrate_7), (\"sequence_8\", sequence_8, substrate_8),\n",
        "    (\"sequence_9\", sequence_9, substrate_9), (\"sequence_10\", sequence_10, substrate_10),\n",
        "    (\"sequence_11\", sequence_11, substrate_11), (\"sequence_12\", sequence_12, substrate_12),\n",
        "    (\"sequence_13\", sequence_13, substrate_13), (\"sequence_14\", sequence_14, substrate_14),\n",
        "    (\"sequence_15\", sequence_15, substrate_15), (\"sequence_16\", sequence_16, substrate_16),\n",
        "    (\"sequence_17\", sequence_17, substrate_17), (\"sequence_18\", sequence_18, substrate_18),\n",
        "    (\"sequence_19\", sequence_19, substrate_19), (\"sequence_20\", sequence_20, substrate_20)\n",
        "]\n",
        "\n",
        "# Filter out entries where both sequence and substrate are longer than 1\n",
        "filtered_data = [(name, seq, subs) for name, seq, subs in input_data if len(seq) > 1 and len(subs) > 1]\n",
        "\n",
        "# Prepare data for ESM-2: Use only the sequence part for ESM-2 processing\n",
        "esm_data = [(name, seq) for name, seq, subs in filtered_data]\n",
        "\n",
        "# Convert batch using ESM's batch converter\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(esm_data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "# Extract ESM-2 representations\n",
        "esm_representations = []\n",
        "with torch.no_grad():\n",
        "    esm_results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "    esm_token_representations = esm_results[\"representations\"][33]\n",
        "\n",
        "# Generate per-sequence ESM-2 representations via averaging (1280-dimensional vectors)\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "    esm_seq_repr = esm_token_representations[i, 1:tokens_len - 1].mean(0).cpu().numpy()\n",
        "    esm_representations.append(esm_seq_repr)\n",
        "\n",
        "# Convert ESM-2 representations to a NumPy array and normalize based on original train set statistics\n",
        "esm_representations = np.array(esm_representations)\n",
        "esm_mean = -0.0006011285004206002\n",
        "esm_std = 0.18902993202209473\n",
        "esm_representations = (esm_representations - esm_mean) / esm_std\n",
        "\n",
        "# Prepare data for ChemBERT: Use only the substrate part for ChemBERT processing\n",
        "chembert_representations = []\n",
        "for _, _, subs in filtered_data:\n",
        "    chembert_inputs = chembert_tokenizer(subs, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        chembert_outputs = chembert_model(**chembert_inputs)\n",
        "    chembert_seq_repr = chembert_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    chembert_representations.append(chembert_seq_repr)\n",
        "\n",
        "# Convert ChemBERT representations to a NumPy array and normalize based on original train set statistics\n",
        "chembert_representations = np.array(chembert_representations)\n",
        "chembert_mean = -0.00015002528380136937\n",
        "chembert_std = 0.6113553047180176\n",
        "chembert_representations = (chembert_representations - chembert_mean) / chembert_std\n",
        "\n",
        "# Concatenate normalized ESM-2 and ChemBERT representations\n",
        "sequence_representations = np.concatenate([esm_representations, chembert_representations], axis=1)\n",
        "\n",
        "\n",
        "# Load the XGBoost model from a pickle file\n",
        "with open('xgb_y1_model.pkl', 'rb') as file:\n",
        "    xgb_y1 = pickle.load(file)\n",
        "predictions_y1 = xgb_y1.predict(sequence_representations)\n",
        "\n",
        "\n",
        "with open('xgb_y2_model.pkl', 'rb') as file:\n",
        "    xgb_y2 = pickle.load(file)\n",
        "predictions_y2 = xgb_y2.predict(sequence_representations)\n",
        "\n",
        "\n",
        "# Define low and high value ranges for each class (0 to 8)\n",
        "class_ranges_y1 = {\n",
        "    0: {\"low\": 0.0, \"high\": 3.32e-8},    # Example values; adjust as needed\n",
        "    1: {\"low\": 3.33e-8, \"high\": 1.0e-2},\n",
        "    2: {\"low\": 1.01e-2, \"high\": 1.0e-1},\n",
        "    3: {\"low\": 1.01e-1, \"high\": 1.0},\n",
        "    4: {\"low\": 1.001, \"high\": 10.0},\n",
        "    5: {\"low\": 1.004e1, \"high\": 1.0e2},\n",
        "    6: {\"low\": 1.0025e2, \"high\": 1.0e3},\n",
        "    7: {\"low\": 1.002e3, \"high\": 7.0e7},\n",
        "}\n",
        "\n",
        "class_ranges_y2 = {\n",
        "    0: {\"low\": 1.0e-10, \"high\": 1.0e-5},    # Example values; adjust as needed\n",
        "    1: {\"low\": 1.01e-5, \"high\": 1.0e-4},\n",
        "    2: {\"low\": 1.002e-4, \"high\": 1.0e-3},\n",
        "    3: {\"low\": 1.002e-3, \"high\": 1.0e-2},\n",
        "    4: {\"low\": 1.008e-2, \"high\": 1.0e-1},\n",
        "    5: {\"low\": 1.01e-1, \"high\": 1.02e2},\n",
        "}\n",
        "\n",
        "\n",
        "# Assuming `predictions` contains the predicted classes from the model (0 to 8)\n",
        "predictions_y1 = xgb_y1.predict(sequence_representations)\n",
        "predictions_y2 = xgb_y2.predict(sequence_representations)\n",
        "# Display the low and high values for each predicted class\n",
        "for i, pred_class in enumerate(predictions_y1):\n",
        "    low = class_ranges_y1[pred_class][\"low\"]\n",
        "    high = class_ranges_y1[pred_class][\"high\"]\n",
        "    print(f\"Sample {i + 1}: Predicted Class for Kcat = {pred_class}, Range for Kcat = [{low}, {high}]\")\n",
        "\n",
        "for i, pred_class in enumerate(predictions_y2):\n",
        "    low = class_ranges_y2[pred_class][\"low\"]\n",
        "    high = class_ranges_y2[pred_class][\"high\"]\n",
        "    print(f\"Sample {i + 1}: Predicted Class for Km = {pred_class}, Range for Km = [{low}, {high}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvhqgnzG_Cw2",
        "outputId": "f102fd1a-9e1c-4ff0-d4b9-05fd0a10d7e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.10/dist-packages (2.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [00:09:56] WARNING: /workspace/src/gbm/gbtree.cc:363: \n",
            "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
            "  machine. Consider using `save_model/load_model` instead. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
            "\n",
            "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [00:09:56] WARNING: /workspace/src/gbm/gbtree.cc:388: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [00:09:56] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [00:09:56] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: Predicted Class for Kcat = 0, Range for Kcat = [0.0, 3.32e-08]\n",
            "Sample 1: Predicted Class for Km = 2, Range for Km = [0.0001002, 0.001]\n"
          ]
        }
      ]
    }
  ]
}