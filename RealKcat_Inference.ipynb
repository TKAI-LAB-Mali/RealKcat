{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Robust Prediction of Enzyme Variant Kinetics with RealKcat**\n",
        "This notebook predicts enzyme kinetics parameters ($k_{\\text{cat}}$ and $K_M$) for given protein sequences and substrates using RealKcat model.\n",
        "\n",
        "**Workflow:**\n",
        "1. **Install Dependencies and Download Models**: Set up required libraries and ensure trained model files are available locally (takes ~2 minute).\n",
        "2. **Choose Mode**: Select `Demo`, `Interactive`, `Bulk`, or `Bulk-large`.\n",
        "3. **Run Inference**: Provide input (or upload CSV) and generate predictions.\n",
        "4. **Save Results**: Download predictions as a CSV file if prompted, bulk mode automatically downloads a csv file with predictions when done.\n",
        "\n",
        "---\n",
        "\n",
        "**How to Navigate (few clicks):**\n",
        "- Start by running the two cells sequentially.\n",
        "- Select an inference mode from the dropdown.\n",
        "- Follow on-screen prompts for input.\n",
        "- View results below each executed cell.\n",
        "\n",
        "_No coding experience is required! Keep sections cells collapsed to keep the notebook organized._"
      ],
      "metadata": {
        "id": "BPpF3-QZGKjT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "URkO_dAKOjl9",
        "outputId": "37965c06-c950-4f69-bb70-fd02e4e9be27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights not found locally. Downloading...\n",
            "--2024-12-06 22:44:00--  https://chowdhurylab.github.io/assets/database/WT_MD_database/model_weights.zip\n",
            "Resolving chowdhurylab.github.io (chowdhurylab.github.io)... 185.199.109.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to chowdhurylab.github.io (chowdhurylab.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69295457 (66M) [application/zip]\n",
            "Saving to: ‘model_weights.zip’\n",
            "\n",
            "model_weights.zip   100%[===================>]  66.08M   199MB/s    in 0.3s    \n",
            "\n",
            "2024-12-06 22:44:01 (199 MB/s) - ‘model_weights.zip’ saved [69295457/69295457]\n",
            "\n",
            "Archive:  model_weights.zip\n",
            "   creating: model_weights/\n",
            "  inflating: model_weights/kcat_model.pkl  \n",
            "  inflating: model_weights/km_model.pkl  \n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies, download and unzip model weights, get dependencies  [~2 minute]\n",
        "!pip install transformers fair-esm torch torchvision torchaudio --quiet\n",
        "import os\n",
        "kcat_model_path = \"model_weights/kcat_model.pkl\"\n",
        "km_model_path = \"model_weights/km_model.pkl\"\n",
        "if not (os.path.exists(kcat_model_path) and os.path.exists(km_model_path)):\n",
        "    print(\"Model weights not found locally. Downloading...\")\n",
        "    !wget https://chowdhurylab.github.io/assets/database/WT_MD_database/model_weights.zip -O model_weights.zip\n",
        "    !unzip -o model_weights.zip\n",
        "else:\n",
        "    print(\"Model weights already exist locally. Skipping download.\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import esm\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "from IPython.display import display, Latex\n",
        "\n",
        "# Helper function to convert floats to \"number x10^(exponent)\" format.\n",
        "def format_sci(value):\n",
        "    # Format with standard scientific notation\n",
        "    s = f\"{value:.2e}\"  # e.g., 3.32e-08\n",
        "    if 'e' in s:\n",
        "        base, exp = s.split('e')\n",
        "        exp = int(exp)\n",
        "        # Replace the standard 'e' notation with 'x10^'\n",
        "        # If exponent is negative: \"3.32e-08\" => \"3.32x10^-8\"\n",
        "        # If exponent is positive: \"3.32e+07\" => \"3.32x10^7\"\n",
        "        return f\"{base}x10^{exp}\"\n",
        "    else:\n",
        "        # For non-exponential numbers, just return them as is\n",
        "        return s\n",
        "# Utility functions for reproducibility\n",
        "def check_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Custom dataset class\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "    def get_labels(self):\n",
        "        return self.labels\n",
        "\n",
        "def dataset_to_tensors(dataset):\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "    data, labels = next(iter(loader))\n",
        "    return data, labels\n",
        "\n",
        "def standardize_x_global_separate(data, global_mean_1, global_std_1, global_mean_2, global_std_2):\n",
        "    X1, X2 = data[:, :1280], data[:, 1280:]\n",
        "    global_std_1, global_std_2 = torch.clamp(global_std_1, min=1e-7), torch.clamp(global_std_2, min=1e-7)\n",
        "    X1_standardized = (X1 - global_mean_1) / global_std_1\n",
        "    X2_standardized = (X2 - global_mean_2) / global_std_2\n",
        "    return torch.cat((X1_standardized, X2_standardized), dim=1).squeeze(1)\n",
        "\n",
        "class StandardizedDatasetGlobalSeparate(Dataset):\n",
        "    def __init__(self, subset, global_mean_1, global_std_1, global_mean_2, global_std_2):\n",
        "        self.subset = subset\n",
        "        self.global_mean_1 = global_mean_1\n",
        "        self.global_std_1 = global_std_1\n",
        "        self.global_mean_2 = global_mean_2\n",
        "        self.global_std_2 = global_std_2\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y1 = self.subset[idx]\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(1)\n",
        "        x_standardized = standardize_x_global_separate(x, self.global_mean_1, self.global_std_1, self.global_mean_2, self.global_std_2)\n",
        "        return x_standardized, y1\n",
        "\n",
        "def apply_global_standardization_separate(dataset, global_mean_1, global_std_1, global_mean_2, global_std_2):\n",
        "    return StandardizedDatasetGlobalSeparate(dataset, global_mean_1, global_std_1, global_mean_2, global_std_2)\n",
        "\n",
        "\n",
        "def load_esm2_model(device, work_dir=\".\"):\n",
        "    model_url = \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\"\n",
        "    model_filename = model_url.split(\"/\")[-1]\n",
        "    model_path = os.path.join(work_dir, model_filename)\n",
        "\n",
        "    # Check if we already have the model weights locally\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Downloading ESM2 model weights...\")\n",
        "        # Download and load model_data from URL\n",
        "        model_data = torch.hub.load_state_dict_from_url(\n",
        "            model_url,\n",
        "            progress=True,\n",
        "            map_location=device\n",
        "        )\n",
        "        # Save model_data for future runs\n",
        "        torch.save(model_data, model_path)\n",
        "    else:\n",
        "        print(\"ESM2 model weights found locally. Loading from disk...\")\n",
        "        model_data = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Now model_data is defined, we can load the ESM model\n",
        "    esm_model, alphabet = esm.pretrained.load_model_and_alphabet_core(\"esm2_t33_650M_UR50D\", model_data)\n",
        "    esm_model.eval().to(device)\n",
        "    return esm_model, alphabet\n",
        "\n",
        "\n",
        "# @title KcatInference Class Definition\n",
        "\n",
        "class KcatInference:\n",
        "    def __init__(self, model_path, device=None):\n",
        "        self.device = device if device else check_device()\n",
        "        self.model = joblib.load(model_path)  # Load the model\n",
        "        set_seed(42)\n",
        "        self.X_test_tensor = None\n",
        "        self.y1_test_tensor = None\n",
        "\n",
        "    def load_data_from_pairs(self, sequence_substrate_pairs):\n",
        "        # sequence_substrate_pairs should be a list of (sequence, SMILES)\n",
        "        # print(f\"Processing {len(sequence_substrate_pairs)} samples...\")\n",
        "        embeddings_list = []\n",
        "        long_chain = []\n",
        "        # print(\"Loading ESM model...\")\n",
        "        esm_model, alphabet = load_esm2_model(self.device)\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "        esm_model.eval().to(self.device)\n",
        "        # Load ChemBERTa\n",
        "        # print(\"Loading ChemBERTa model...\")\n",
        "        chemberta_tokenizer = AutoTokenizer.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
        "        chemberta_model = AutoModel.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
        "        chemberta_model.eval().to(self.device)\n",
        "        print(\"Models loaded. Embedding sequences and substrates...\")\n",
        "\n",
        "        for index, (sequence, substrate) in enumerate(sequence_substrate_pairs, start=1):\n",
        "            if not sequence or not substrate:\n",
        "                print(f\"Skipping sample {index} due to missing sequence or substrate.\")\n",
        "                continue\n",
        "            if len(sequence) > 1022:\n",
        "                long_chain.append(index)\n",
        "                print(f\"Skipping sample {index} due to excessive sequence length.\")\n",
        "                continue\n",
        "            # ESM embedding\n",
        "            try:\n",
        "                seq = (f'sample_{index}', sequence)\n",
        "                batch_labels, batch_strs, batch_tokens = batch_converter([seq])\n",
        "                batch_tokens = batch_tokens.to(self.device)\n",
        "                batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "                with torch.no_grad():\n",
        "                    results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "                token_representations = results[\"representations\"][33][0, 1 : batch_lens - 1]\n",
        "                embedded_sequence = token_representations.mean(dim=0).type(torch.float32)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sequence at sample {index}: {e}\")\n",
        "                continue\n",
        "            # ChemBERTa embedding\n",
        "            try:\n",
        "                smiles = [substrate]\n",
        "                inputs = chemberta_tokenizer(smiles, return_tensors='pt', padding=True, truncation=False)\n",
        "                inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "                if inputs['input_ids'].shape[1] > 512:\n",
        "                    print(f\"Skipping sample {index} due to SMILES length exceeding model capacity.\")\n",
        "                    continue\n",
        "                with torch.no_grad():\n",
        "                    outputs = chemberta_model(**inputs)\n",
        "                last_hidden_state = outputs.last_hidden_state\n",
        "                chemberta_embedding = torch.mean(last_hidden_state, dim=1).squeeze(0).type(torch.float32)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing SMILES at sample {index}: {e}\")\n",
        "                continue\n",
        "            # Flatten embeddings if needed\n",
        "            if embedded_sequence.dim() > 1:\n",
        "                embedded_sequence = embedded_sequence.flatten()\n",
        "            if chemberta_embedding.dim() > 1:\n",
        "                chemberta_embedding = chemberta_embedding.flatten()\n",
        "            feature = torch.cat((embedded_sequence, chemberta_embedding))\n",
        "            embeddings_list.append(feature)\n",
        "        if len(embeddings_list) == 0:\n",
        "            raise ValueError(\"No valid samples were processed.\")\n",
        "        print(f\"Total valid samples processed: {len(embeddings_list)}\")\n",
        "        self.X_test_tensor = torch.stack(embeddings_list).to(self.device)\n",
        "        self.y1_test_tensor = torch.zeros(len(embeddings_list), dtype=torch.long).to(self.device)\n",
        "    def standardize_test_data(self, global_mean_1, global_std_1, global_mean_2, global_std_2):\n",
        "        dataset = TensorDataset(self.X_test_tensor, self.y1_test_tensor)\n",
        "        self.test_dataset_std = apply_global_standardization_separate(dataset, global_mean_1, global_std_1, global_mean_2, global_std_2)\n",
        "    def convert_to_numpy(self):\n",
        "        X_test_data, test_y1 = dataset_to_tensors(self.test_dataset_std)\n",
        "        return X_test_data.cpu().numpy(), test_y1.cpu().numpy()\n",
        "    def predict(self, X_test_data):\n",
        "        return self.model.predict(X_test_data)\n",
        "    def display_prediction_ranges_kcat(self, predictions, class_ranges):\n",
        "        # print(\"\\n=== kcat Prediction Results ===\")\n",
        "        for i, pred_class in enumerate(predictions):\n",
        "            low, high = class_ranges[pred_class][\"low\"], class_ranges[pred_class][\"high\"]\n",
        "            low_str = format_sci(low)\n",
        "            high_str = format_sci(high)\n",
        "            print(f\"Sample {i + 1}: Predicted Class = {pred_class}, kcat range = [{low_str}, {high_str}]\")\n",
        "    def display_prediction_ranges_km(self, predictions, class_ranges):\n",
        "        # print(\"\\n=== Km Prediction Results ===\")\n",
        "        for i, pred_class in enumerate(predictions):\n",
        "            low, high = class_ranges[pred_class][\"low\"], class_ranges[pred_class][\"high\"]\n",
        "            low_str = format_sci(low)\n",
        "            high_str = format_sci(high)\n",
        "            print(f\"Sample {i + 1}: Predicted Class = {pred_class}, km range = [{low_str}, {high_str}]\")\n",
        "\n",
        "# #@title Configuration of Standardization Parameters and Class Ranges\n",
        "device = check_device()\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Global standardization params (from training)\n",
        "global_mean_1 = torch.tensor(-0.0006011285004206002, device=device)\n",
        "global_std_1 = torch.tensor(0.18902993202209473, device=device)\n",
        "global_mean_2 = torch.tensor(-0.00015002528380136937, device=device)\n",
        "global_std_2 = torch.tensor(0.6113553047180176, device=device)\n",
        "\n",
        "class_ranges_kcat = {\n",
        "    0: {\"low\": 0.0, \"high\": 3.32e-8},\n",
        "    1: {\"low\": 3.33e-8, \"high\": 1.0e-2},\n",
        "    2: {\"low\": 1.01e-2, \"high\": 1.0e-1},\n",
        "    3: {\"low\": 1.01e-1, \"high\": 1.0},\n",
        "    4: {\"low\": 1.001, \"high\": 10.0},\n",
        "    5: {\"low\": 1.004e1, \"high\": 1.0e2},\n",
        "    6: {\"low\": 1.0025e2, \"high\": 1.0e3},\n",
        "    7: {\"low\": 1.002e3, \"high\": 7.0e7}\n",
        "}\n",
        "\n",
        "class_ranges_km = {\n",
        "    0: {\"low\": 1.0e-10, \"high\": 1.0e-5},\n",
        "    1: {\"low\": 1.01e-5, \"high\": 1.0e-4},\n",
        "    2: {\"low\": 1.002e-4, \"high\": 1.0e-3},\n",
        "    3: {\"low\": 1.002e-3, \"high\": 1.0e-2},\n",
        "    4: {\"low\": 1.008e-2, \"high\": 1.0e-1},\n",
        "    5: {\"low\": 1.01e-1, \"high\": 1.02e2},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choose RealKcat Mode of Inference: $k_{cat}$ and $K_{M}$**\n",
        "\n",
        "**[Demo takes ~2 minutes]**\n",
        "\n",
        "Notes:\n",
        "- **Demo**: For testing purposes with predefined inputs. Runs in ~2 minutes.\n",
        "- **Interactive**: Allows manual entry of sequences and SMILES for flexible input.\n",
        "- **Bulk**: Processes uploaded CSV files, ideal for < 100 sequence-substrate pairs.\n",
        "- **Bulk-large**: Optimized for processing very large datasets with lower memory consumption in batches of size 100 for memory efficiency and slower speed.\n"
      ],
      "metadata": {
        "id": "UXXBQVU3DQwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose RealKcat Mode of Inference,  $k_{cat}$ and $K_{M}$  [Demo  takes ~2 minutes]\n",
        "mode = \"Demo\"  #@param [\"Demo\", \"Interactive\", \"Bulk\", \"Bulk-large\"]\n",
        "\n",
        "if mode == \"Bulk\":\n",
        "    print(\"Please upload a CSV file containing 'sequence' and 'Isomeric SMILES' columns.\")\n",
        "    uploaded = files.upload()\n",
        "    csv_filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # Load CSV and prepare data\n",
        "    df = pd.read_csv(csv_filename)\n",
        "    sequence_substrate_pairs = list(zip(df['sequence'], df['Isomeric SMILES']))\n",
        "\n",
        "    # Generate embeddings using KcatInference (just as a convenient class)\n",
        "    temp_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "    temp_inference.load_data_from_pairs(sequence_substrate_pairs)\n",
        "    temp_inference.standardize_test_data(global_mean_1, global_std_1, global_mean_2, global_std_2)\n",
        "    X_test, _ = temp_inference.convert_to_numpy()\n",
        "\n",
        "    # Kcat predictions\n",
        "    kcat_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "    y_pred_kcat = kcat_inference.predict(X_test)\n",
        "\n",
        "    # KM predictions\n",
        "    km_inference = KcatInference(model_path=km_model_path, device=device)\n",
        "    y_pred_km = km_inference.predict(X_test)\n",
        "\n",
        "    # Map predictions to ranges\n",
        "    kcat_low = [class_ranges_kcat[p][\"low\"] for p in y_pred_kcat]\n",
        "    kcat_high = [class_ranges_kcat[p][\"high\"] for p in y_pred_kcat]\n",
        "    km_low = [class_ranges_km[p][\"low\"] for p in y_pred_km]\n",
        "    km_high = [class_ranges_km[p][\"high\"] for p in y_pred_km]\n",
        "\n",
        "    # Convert to formatted strings\n",
        "    kcat_low_str = [format_sci(val) for val in kcat_low]\n",
        "    kcat_high_str = [format_sci(val) for val in kcat_high]\n",
        "    km_low_str = [format_sci(val) for val in km_low]\n",
        "    km_high_str = [format_sci(val) for val in km_high]\n",
        "\n",
        "    df['Predicted_Kcat_low'] = kcat_low_str\n",
        "    df['Predicted_Kcat_high'] = kcat_high_str\n",
        "    df['Predicted_KM_low'] = km_low_str\n",
        "    df['Predicted_KM_high'] = km_high_str\n",
        "\n",
        "    # Save updated CSV\n",
        "    output_csv = \"inference_results.csv\"\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(\"Inference complete. The updated results are saved in 'inference_results.csv'.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    files.download(output_csv)\n",
        "\n",
        "elif mode == \"Bulk-large\":\n",
        "    print(\"Please upload a CSV file containing 'sequence' and 'Isomeric SMILES' columns.\")\n",
        "    uploaded = files.upload()\n",
        "    csv_filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # Load CSV and prepare data\n",
        "    df = pd.read_csv(csv_filename)\n",
        "    sequence_substrate_pairs = list(zip(df['sequence'], df['Isomeric SMILES']))\n",
        "\n",
        "    # Batch size for memory-efficient processing\n",
        "    batch_size = 100\n",
        "    total_samples = len(sequence_substrate_pairs)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ceiling division\n",
        "\n",
        "    # Placeholder for predictions\n",
        "    kcat_low_list, kcat_high_list = [], []\n",
        "    km_low_list, km_high_list = [], []\n",
        "\n",
        "    print(f\"Processing {total_samples} samples in batches of {batch_size}...\")\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        # Determine batch start and end indices\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        # Extract the batch\n",
        "        batch_pairs = sequence_substrate_pairs[start_idx:end_idx]\n",
        "        print(f\"Processing batch {batch_idx + 1}/{num_batches} (samples {start_idx + 1}-{end_idx})...\")\n",
        "\n",
        "        # Generate embeddings using KcatInference for the current batch\n",
        "        temp_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "        temp_inference.load_data_from_pairs(batch_pairs)\n",
        "        temp_inference.standardize_test_data(global_mean_1, global_std_1, global_mean_2, global_std_2)\n",
        "        X_test, _ = temp_inference.convert_to_numpy()\n",
        "\n",
        "        # Kcat predictions for the batch\n",
        "        kcat_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "        y_pred_kcat = kcat_inference.predict(X_test)\n",
        "\n",
        "        # KM predictions for the batch\n",
        "        km_inference = KcatInference(model_path=km_model_path, device=device)\n",
        "        y_pred_km = km_inference.predict(X_test)\n",
        "\n",
        "        # Map predictions to ranges\n",
        "        kcat_low = [class_ranges_kcat[p][\"low\"] for p in y_pred_kcat]\n",
        "        kcat_high = [class_ranges_kcat[p][\"high\"] for p in y_pred_kcat]\n",
        "        km_low = [class_ranges_km[p][\"low\"] for p in y_pred_km]\n",
        "        km_high = [class_ranges_km[p][\"high\"] for p in y_pred_km]\n",
        "\n",
        "        # Append results to the respective lists\n",
        "        kcat_low_list.extend(kcat_low)\n",
        "        kcat_high_list.extend(kcat_high)\n",
        "        km_low_list.extend(km_low)\n",
        "        km_high_list.extend(km_high)\n",
        "\n",
        "    # Convert ranges to formatted strings\n",
        "    kcat_low_str = [format_sci(val) for val in kcat_low_list]\n",
        "    kcat_high_str = [format_sci(val) for val in kcat_high_list]\n",
        "    km_low_str = [format_sci(val) for val in km_low_list]\n",
        "    km_high_str = [format_sci(val) for val in km_high_list]\n",
        "\n",
        "    # Add predictions to the DataFrame\n",
        "    df['Predicted_Kcat_low'] = kcat_low_str\n",
        "    df['Predicted_Kcat_high'] = kcat_high_str\n",
        "    df['Predicted_KM_low'] = km_low_str\n",
        "    df['Predicted_KM_high'] = km_high_str\n",
        "\n",
        "    # Save updated CSV\n",
        "    output_csv = \"inference_results.csv\"\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(\"Inference complete. The updated results are saved in 'inference_results.csv'.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    files.download(output_csv)\n",
        "\n",
        "elif mode == \"Interactive\" or mode == \"Demo\":\n",
        "    sequence_substrate_pairs = []\n",
        "    max_entries = 10\n",
        "    count = 0\n",
        "\n",
        "    if mode == \"Demo\":\n",
        "        print(\"\\nRunning in Demo Mode...\")\n",
        "        # Predefined sequence and SMILES for demo\n",
        "        sequence_substrate_pairs = [\n",
        "            (\n",
        "                \"MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDLSHIPTAVKIKGFSGEDATPALEGADVVLISAGVARKPGMDRSDLFNVNAGIVKNLVQQVAKTCPKACIGIITNPVNTTVAIAAEVLKKAGVYDKNKLFGVTTLDIIRSNTFVAELKGKQPGEVEVPVIGGHSGVTILPLLSQVPGVSFTEQEVADLTKRIQNAGTEVVEAKAGGGSATLSMGQAAARFGLSLVRALQGEQGVVECAYVEGDGQYARFFSQPLLLGKNGVEERKSIGTLSAFEQNALEGMLDTLKKDIALGEEFVNK\",\n",
        "                \"C(C(=O)C(=O)O)C(=O)O\",\n",
        "            ),\n",
        "            (\n",
        "                \"MGVEQILKRKTGVIVGEDVHNLFTYAKEHKFAIPAINVTSSSTAVAALEAARDSKSPIILQTSNGGAAYFAGKGISNEGQNASIKGAIAAAHYIRSIAPAYGIPVVLHSDHCAKKLLPWFDGMLEADEAYFKEHGEPLFSSHMLDLSEETDEENISTCVKYFKRMAAMDQWLEMEIGITGGEEDGVNNENADKEDLYTKPEQVYNVYKALHPISPNFSIAAAFGNCHGLYAGDIALRPEILAEHQKYTREQVGCKEEKPLFLVFHGGSGSTVQEFHTGIDNGVVKVNLDTDCQYAYLTGIRDYVLNKKDYIMSPVGNPEGPEKPNKKFFDPRVWVREGEKTMGAKITKSLETFRTTNTL\",\n",
        "                \"C([C@H](C=O)O)OP(=O)(O)O\",\n",
        "            ),\n",
        "        ]\n",
        "        count = len(sequence_substrate_pairs)\n",
        "\n",
        "    else:\n",
        "      while count < max_entries:\n",
        "          print(f\"\\nEntry {count+1}/{max_entries}:\")\n",
        "          user_sequence = input(\"Enter Sequence (or press Enter to stop): \").strip()\n",
        "          if user_sequence == \"\":\n",
        "              # user pressed Enter without input -> stop\n",
        "              break\n",
        "          user_smiles = input(\"Enter Isomeric SMILES: \").strip()\n",
        "          if user_smiles == \"\":\n",
        "              # If SMILES is empty, skip this entry\n",
        "              print(\"No SMILES provided. Skipping this entry.\")\n",
        "              continue\n",
        "\n",
        "          sequence_substrate_pairs.append((user_sequence, user_smiles))\n",
        "          count += 1\n",
        "\n",
        "          # Optionally ask if user wants to add another entry\n",
        "          if count < max_entries:\n",
        "              cont = input(\"Add another? (y/n): \").strip().lower()\n",
        "              if cont not in [\"y\", \"yes\"]:\n",
        "                  break\n",
        "\n",
        "    if len(sequence_substrate_pairs) == 0:\n",
        "        print(\"No entries provided. Exiting.\")\n",
        "    else:\n",
        "        # Now run inference as done in bulk mode\n",
        "        print(f\"Processing {len(sequence_substrate_pairs)} samples...\")\n",
        "\n",
        "        # Generate embeddings (from previously defined logic)\n",
        "        temp_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "        temp_inference.load_data_from_pairs(sequence_substrate_pairs)\n",
        "        temp_inference.standardize_test_data(global_mean_1, global_std_1, global_mean_2, global_std_2)\n",
        "        X_test, _ = temp_inference.convert_to_numpy()\n",
        "\n",
        "        # Kcat predictions\n",
        "        kcat_inference = KcatInference(model_path=kcat_model_path, device=device)\n",
        "        y_pred_kcat = kcat_inference.predict(X_test)\n",
        "\n",
        "        # KM predictions\n",
        "        km_inference = KcatInference(model_path=km_model_path, device=device)\n",
        "        y_pred_km = km_inference.predict(X_test)\n",
        "\n",
        "        # Display predictions\n",
        "        display(Latex(r\"\\textbf{\\large $k_{\\text{cat}}$ Prediction Results:}\"))\n",
        "        kcat_inference.display_prediction_ranges_kcat(y_pred_kcat, class_ranges_kcat)\n",
        "\n",
        "        display(Latex(r\"\\textbf{\\large $K_{M}$ Prediction Results:}\"))\n",
        "        km_inference.display_prediction_ranges_km(y_pred_km, class_ranges_km)\n",
        "\n",
        "        # Optional save to CSV\n",
        "        if mode == \"Interactive\":\n",
        "          save_results = input(\"Save results to CSV? (y/n): \").strip().lower()\n",
        "          if save_results in [\"y\", \"yes\"]:\n",
        "              df = pd.DataFrame(sequence_substrate_pairs, columns=[\"sequence\", \"SMILES\"])\n",
        "              kcat_low = [class_ranges_kcat[p][\"low\"] for p in y_pred_kcat]\n",
        "              kcat_high = [class_ranges_kcat[p][\"high\"] for p in y_pred_kcat]\n",
        "              km_low = [class_ranges_km[p][\"low\"] for p in y_pred_km]\n",
        "              km_high = [class_ranges_km[p][\"high\"] for p in y_pred_km]\n",
        "              df[\"Kcat_low\"] = [format_sci(val) for val in kcat_low]\n",
        "              df[\"Kcat_high\"] = [format_sci(val) for val in kcat_high]\n",
        "              df[\"KM_low\"] = [format_sci(val) for val in km_low]\n",
        "              df[\"KM_high\"] = [format_sci(val) for val in km_high]\n",
        "              df.to_csv(\"interactive_results.csv\", index=False)\n",
        "              print(\"Results saved to 'interactive_results.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "cellView": "form",
        "id": "XGMrWhdZPAs9",
        "outputId": "20a8e6a8-21dd-477a-95d6-2b9f40f5ab53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running in Demo Mode...\n",
            "Processing 2 samples...\n",
            "Downloading ESM2 model weights...\n",
            "Models loaded. Embedding sequences and substrates...\n",
            "Total valid samples processed: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "\\textbf{\\large $k_{\\text{cat}}$ Prediction Results:}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: Predicted Class = 6, kcat range = [1.00x10^2, 1.00x10^3]\n",
            "Sample 2: Predicted Class = 5, kcat range = [1.00x10^1, 1.00x10^2]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "\\textbf{\\large $K_{M}$ Prediction Results:}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: Predicted Class = 1, km range = [1.01x10^-5, 1.00x10^-4]\n",
            "Sample 2: Predicted Class = 2, km range = [1.00x10^-4, 1.00x10^-3]\n"
          ]
        }
      ]
    }
  ]
}